{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os.path\n",
    "import psycopg2\n",
    "from gensim.models import Doc2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "import linecache\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Dropout, Activation, Input, Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "from keras.utils import np_utils, Sequence\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import keras as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_vec_dim = 88\n",
    "embedding_dim = 6\n",
    "dim = 128\n",
    "keep_prob = 0.6\n",
    "\n",
    "batch_size = 256\n",
    "test_size = 256\n",
    "\n",
    "\n",
    "# disable tensorflow debugging information\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "logdir = '/tmp/logs'\n",
    "\n",
    "kernel_init = K.initializers.VarianceScaling(scale=1.0, mode='fan_avg',\n",
    "                                             distribution='uniform')\n",
    "bias_init = K.initializers.Constant(value=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = pd.read_csv('E:\\k-yokoi\\IJaDataset2.0-full-functions2\\\\functions.csv', names=('id', 'document'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = psycopg2.connect(\"host=localhost port=5432 dbname=bigclonebench user=postgres password=1994MomJul\")\n",
    "connection.get_backend_pid()\n",
    "cur=connection.cursor()\n",
    "sql = 'select function_id_one, function_id_two from clones where min_size >= 5'\n",
    "cur.execute(sql)\n",
    "\n",
    "clones = [row for row in cur]\n",
    "\n",
    "\n",
    "cur.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_id_set = set(functions['id'])\n",
    "clones = [row for row in clones if row[0] in function_id_set and row[1] in function_id_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = psycopg2.connect(\"host=localhost port=5432 dbname=bigclonebench user=postgres password=1994MomJul\")\n",
    "connection.get_backend_pid()\n",
    "cur=connection.cursor()\n",
    "sql = \"SELECT FP.function_id_one, FP.function_id_two from false_positives as FP, functions as A, functions as B \"\n",
    "sql += \"where FP.function_id_one=A.id and FP.function_id_two=B.id and \"\n",
    "sql += \"A.normalized_size>=5 and B.normalized_size>=5\"\n",
    "cur.execute(sql)\n",
    "\n",
    "FP = [row for row in cur]\n",
    "\n",
    "\n",
    "cur.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_id_set = set(functions['id'])\n",
    "FP = [row for row in FP if row[0] in function_id_set and row[1] in function_id_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clones= pd.DataFrame(clones, columns=['id_one', 'id_two'])\n",
    "df_clones['clone'] = 1\n",
    "df_fp= pd.DataFrame(FP, columns=['id_one', 'id_two'])\n",
    "df_fp['clone'] = 0\n",
    "df_clones = df_clones.append(df_fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dv = np.load('model/scdvnoidf.dv.npy')\n",
    "#dv = np.load('model/AvgVec.dv.npy')\n",
    "dv = np.load('model/LSI.dv.npy')\n",
    "#dv = np.load('model/LDA.dv.npy')\n",
    "#docmodel = Doc2Vec.load(\"model/pvdbow.model\")\n",
    "#docmodel = Doc2Vec.load(\"model/pvdm.model\")\n",
    "#dv = docmodel.docvecs.vectors_docs\n",
    "\n",
    "bin_vec_dim = dv.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_dict = {}\n",
    "for i, id in enumerate(functions['id']):\n",
    "    dv_dict[id] = dv[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_left = np.array([dv_dict[id] for id in df_clones['id_one']])\n",
    "X_right = np.array([dv_dict[id] for id in df_clones['id_two']])\n",
    "t = np.array(df_clones['clone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xl, Xr, y = shuffle(X_left, X_right, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "5498651/5498651 [==============================] - 162s 29us/step - loss: 0.0135 - acc: 0.9957\n",
      "610962/610962 [==============================] - 11s 17us/step\n",
      "accuracy: 0.9982, recall: 0.9992, precision: 0.9989, f1 score: 0.9991\n",
      "\n",
      "Epoch 1/1\n",
      "5498651/5498651 [==============================] - 186s 34us/step - loss: 0.0139 - acc: 0.9954\n",
      "610962/610962 [==============================] - 8s 14us/step\n",
      "accuracy: 0.9981, recall: 0.9993, precision: 0.9987, f1 score: 0.9990\n",
      "\n",
      "Epoch 1/1\n",
      "5498651/5498651 [==============================] - 185s 34us/step - loss: 0.0126 - acc: 0.9961\n",
      "610962/610962 [==============================] - 11s 18us/step\n",
      "accuracy: 0.9982, recall: 0.9991, precision: 0.9990, f1 score: 0.9991\n",
      "\n",
      "Epoch 1/1\n",
      "5498652/5498652 [==============================] - 182s 33us/step - loss: 0.0121 - acc: 0.9963\n",
      "610961/610961 [==============================] - 9s 15us/step\n",
      "accuracy: 0.9976, recall: 0.9990, precision: 0.9986, f1 score: 0.9988\n",
      "\n",
      "Epoch 1/1\n",
      "5498652/5498652 [==============================] - 179s 33us/step - loss: 0.0131 - acc: 0.9958\n",
      "610961/610961 [==============================] - 10s 17us/step\n",
      "accuracy: 0.9980, recall: 0.9991, precision: 0.9989, f1 score: 0.9990\n",
      "\n",
      "Epoch 1/1\n",
      "5498652/5498652 [==============================] - 190s 35us/step - loss: 0.0131 - acc: 0.9959\n",
      "610961/610961 [==============================] - 11s 18us/step\n",
      "accuracy: 0.9980, recall: 0.9992, precision: 0.9987, f1 score: 0.9990\n",
      "\n",
      "Epoch 1/1\n",
      "5498652/5498652 [==============================] - 183s 33us/step - loss: 0.0123 - acc: 0.9962\n",
      "610961/610961 [==============================] - 12s 19us/step\n",
      "accuracy: 0.9974, recall: 0.9995, precision: 0.9979, f1 score: 0.9987\n",
      "\n",
      "Epoch 1/1\n",
      "5498652/5498652 [==============================] - 183s 33us/step - loss: 0.0124 - acc: 0.9962\n",
      "610961/610961 [==============================] - 13s 21us/step\n",
      "accuracy: 0.9982, recall: 0.9992, precision: 0.9990, f1 score: 0.9991\n",
      "\n",
      "Epoch 1/1\n",
      "5498652/5498652 [==============================] - 191s 35us/step - loss: 0.0121 - acc: 0.9963\n",
      "610961/610961 [==============================] - 12s 19us/step\n",
      "accuracy: 0.9981, recall: 0.9986, precision: 0.9995, f1 score: 0.9990\n",
      "\n",
      "Epoch 1/1\n",
      "5498652/5498652 [==============================] - 183s 33us/step - loss: 0.0132 - acc: 0.9959\n",
      "610961/610961 [==============================] - 11s 17us/step\n",
      "accuracy: 0.9977, recall: 0.9994, precision: 0.9982, f1 score: 0.9988\n",
      "\n",
      "Avg accuracy: 0.9980, avg recall: 0.9992, avg precision: 0.9987, avg f1 score: 0.9989\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "avg_accuracy = 0.\n",
    "avg_recall = 0.\n",
    "avg_precision = 0.\n",
    "avg_f1_score = 0.\n",
    "skf_split = skf.split(Xl, y)\n",
    "\n",
    "for train_idx, test_idx in skf_split:\n",
    "        train_X_left = Xl[train_idx]\n",
    "        train_X_right = Xr[train_idx]\n",
    "        train_Y = y[train_idx]\n",
    "        \n",
    "        train_X_left, train_X_right, train_Y = shuffle(train_X_left,\n",
    "                                                       train_X_right, train_Y)\n",
    "        \n",
    "        test_X_left = Xl[test_idx]\n",
    "        test_X_right = Xr[test_idx]\n",
    "        test_Y = y[test_idx]\n",
    "        \n",
    "        validate_X_left = test_X_left[:256]\n",
    "        validate_X_right = test_X_right[:256]\n",
    "        validate_Y = test_Y[:256]\n",
    "\n",
    "        X_left = Input(shape=(bin_vec_dim, ))\n",
    "        X_right = Input(shape=(bin_vec_dim, ))\n",
    "\n",
    "        predictions = classification(X_left, X_right)\n",
    "\n",
    "        model = Model(inputs=[X_left, X_right], outputs=predictions)\n",
    "\n",
    "        model.compile(optimizer=K.optimizers.adam(lr=0.001),\n",
    "                      loss=K.losses.binary_crossentropy,\n",
    "                      metrics=['accuracy'])\n",
    "        #samples_generator = SequenceSamples(train_X_left,train_X_right,\n",
    "        #                                    train_Y, batch_size)\n",
    "        model.fit([train_X_left, train_X_right], train_Y,\n",
    "                  epochs=1,verbose=1, batch_size=batch_size)\n",
    "        y_pred = model.predict([test_X_left, test_X_right], verbose=1, batch_size=batch_size)\n",
    "        y_pred = np.round(y_pred)\n",
    "        accuracy = accuracy_score(test_Y, y_pred)\n",
    "        precision, recall, fscore, _ = precision_recall_fscore_support(test_Y,\n",
    "                                                                 y_pred, average='binary')\n",
    "        print(\"accuracy: %.4f, recall: %.4f, \"\n",
    "                   \"precision: %.4f, f1 score: %.4f\\n\" % (\n",
    "                   accuracy, recall, precision, fscore))\n",
    "        avg_accuracy += accuracy\n",
    "        avg_precision += precision\n",
    "        avg_recall += recall\n",
    "        avg_f1_score += fscore\n",
    "        \n",
    "avg_accuracy /= 10.0\n",
    "avg_precision /= 10.0\n",
    "avg_recall /= 10.0\n",
    "avg_f1_score /= 10.0\n",
    "\n",
    "print('Avg accuracy: %.4f, avg recall: %.4f, avg precision: %.4f, avg f1 '\n",
    "          'score: %.4f' % (\n",
    "              avg_accuracy, avg_recall, avg_precision, avg_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "6109613/6109613 [==============================] - 76s 12us/step - loss: 0.0121 - acc: 0.9963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2700b302358>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_left = Xl\n",
    "train_X_right = Xr\n",
    "train_Y = y\n",
    "X_left = Input(shape=(bin_vec_dim, ))\n",
    "X_right = Input(shape=(bin_vec_dim, ))\n",
    "\n",
    "predictions = classification(X_left, X_right)\n",
    "\n",
    "model = Model(inputs=[X_left, X_right], outputs=predictions)\n",
    "\n",
    "model.compile(optimizer=K.optimizers.adam(lr=0.001),\n",
    "              loss=K.losses.binary_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "#samples_generator = SequenceSamples(train_X_left,train_X_right,\n",
    "#                                    train_Y, batch_size)\n",
    "model.fit([train_X_left, train_X_right], train_Y,\n",
    "          epochs=1,verbose=1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/bcbnn.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ok\":true,\"channel\":\"G7LJMJJTE\",\"ts\":\"1545027092.000100\",\"message\":{\"type\":\"message\",\"subtype\":\"bot_message\",\"text\":\"finish\",\"ts\":\"1545027092.000100\",\"username\":\"bot\",\"bot_id\":\"B7LJJJEQ4\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   323    0   190  100   133    608    426 --:--:-- --:--:-- --:--:--  1035\n"
     ]
    }
   ],
   "source": [
    "!slack-post finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(x1, x2):\n",
    "    input = Input(shape=(bin_vec_dim,))\n",
    "    # share layers\n",
    "    #feed_forward_model = Model(inputs=input, outputs=feed_forward(input))\n",
    "    #x1 = feed_forward_model(x1)\n",
    "    #x2 = feed_forward_model(x2)\n",
    "    concat_input = Input(shape=(bin_vec_dim*2,))\n",
    "    # share layers\n",
    "    merge_model = Model(inputs=concat_input,\n",
    "                        outputs=Activation(activation='relu')(\n",
    "                            BatchNormalization()(\n",
    "                                Dense(100, kernel_initializer=kernel_init,\n",
    "                                      bias_initializer=bias_init,\n",
    "                                      input_shape=(bin_vec_dim*2,))(\n",
    "                                    concat_input))))\n",
    "    \n",
    "    xc1 = K.layers.concatenate([x1, x2])\n",
    "    xc1 = merge_model(xc1)\n",
    "    \n",
    "    xc2 = K.layers.concatenate([x2, x1])\n",
    "    xc2 = merge_model(xc2)\n",
    "    \n",
    "    xc = K.layers.average([xc1, xc2])\n",
    "    \n",
    "    x = Dense(1, use_bias=False, activation='sigmoid',\n",
    "              kernel_initializer=kernel_init,\n",
    "              batch_input_shape=K.backend.get_variable_shape(xc))(xc)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
