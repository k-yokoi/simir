{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = pd.read_csv('functions.csv', names=('id', 'document'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[word for word in document.split() ] for document in functions['document']]\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsi model\n",
    "lsi_model = models.LsiModel(corpus=corpus, id2word=dictionary, num_topics=200)\n",
    "lsi_model.save('model/lsi.model')\n",
    "corpus_lsi = lsi_model[corpus]\n",
    "dv = gensim.matutils.corpus2dense(corpus_lsi, num_terms=lsi_model.num_topics).T\n",
    "np.save('model/lsi.dv', dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda model\n",
    "lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=100, dtype=np.float64)\n",
    "corpus_lda = lda_model[corpus]\n",
    "dv = gensim.matutils.corpus2dense(corpus_lda, num_terms=lda_model.num_topics).T\n",
    "np.save('model/lda.dv', dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pvdbow\n",
    "sentences = [TaggedDocument(text, [i]) for i, text in enumerate(texts)]\n",
    "model = Doc2Vec(sentences, dm=0, vector_size=300, window=15, min_count=1, workers=4, epochs=20, sample = 1e-3, seed=1)\n",
    "model.save(\"model/pvdbow.model\")\n",
    "model = Doc2Vec.load(\"model/pvdbow.model\")\n",
    "dv = model.docvecs.vectors_docs\n",
    "np.save('model/pvdbow.dv', dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pvdm\n",
    "sentences = [TaggedDocument(text, [i]) for i, text in enumerate(texts)]\n",
    "model = Doc2Vec(sentences, dm=1, vector_size=300, window=5, min_count=1, workers=4, epochs=20, sample = 1e-3, seed=11)\n",
    "model.save('model/pvdm.model')\n",
    "dv = model.docvecs.vectors_docs\n",
    "np.save('model/pvdm.dv', dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WV-avg\n",
    "num_features = 300\n",
    "sentences = [doc.split() for doc in functions['document']]\n",
    "#model = Word2Vec(sentences, workers=4, hs = 0, sg = 1, negative = 10, iter = 25,size=num_features, min_count = 1, window = 10, sample = 1e-3, seed=1)\n",
    "#model.save(\"model/word2vec.model\")\n",
    "model = Word2Vec.load(\"model/word2vec.model\")\n",
    "wv = model.wv\n",
    "size = model.vector_size\n",
    "dv = []\n",
    "for text in texts:\n",
    "    vec = np.zeros( num_features, dtype=\"float32\" )\n",
    "    for word in text:\n",
    "        vec += wv[word]\n",
    "    norm = np.sqrt(np.einsum('...i,...i', vec, vec))\n",
    "    if(norm!=0):\n",
    "        vec /= norm\n",
    "    dv.append(vec)\n",
    "np.save('model/avgvec.dv', np.array(dv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5610"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions['id'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions[functions['id']==5610].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions[functions['id']==53609].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
